---
title: 28.(nlp) BERT (Bidirectional Encoder Representations from Transformers)
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: Neural Machine Translation, BLEU
---

# BERT (Bidirectional Encoder Representations from Transformers)

( 참고 : "딥러닝을 이용한 자연어 처리 입문" (https://wikidocs.net/book/2155) )

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

목차

1. Introduction to BERT
2. Contextual Embedding
3. BERT의 sub-word Tokenizer : WordPiece
4. Position Embedding
5. Pre-training
   1. Masked Language Model (MLM)
   2. Next Sentence Prediction (NSP)
6. Segment Embedding
7. Fine-tuning
   1. Single Text Classification
   2. Tagging
   3. Text Pair Classification/Regression
   4. Question Answering
8. Attention Mask

<br>

# 1. Introduction to BERT

**What is BERT?**

- Transformer를 사용해서 구현됨

- 위키피디아(25억 단어)와 BooksCorpus(8억 단어)의 Text Data로 학습된 "**pre-trained LM**"

- Label이 없는 text data로 훈련한 모델을 사용하여,

  Label이 있는 다른 task를 풀 수 있다! ( + 추가의 학습 / 튜닝 , called **fine-tuning** )

<br>

**Fine-Tuning **example ?

- Goal :  Spam mail classification

- (pre-trained LM인) BERT 위에, classification을 하기 위한 layer를 추가!

  $$\rightarrow$$ BERT가 학습 과정에서 얻은 지식을 활용 가능!

<img src="https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG" width="650" />..

<br>

**Details of BERT**

- structure : Transformer의 Encoder로 구성

- BERT-Base : $$L=12, D=768, A=12$$: 110M개의 파라미터

  BERT-Large : $$L=24, D=1024, A=16$$ : 340M개의 파라미터

  where $$L$$ = transformer의 encoder 개수, $$D$$ = $$d_{model}$$ , $$A$$ = self-attention의 head 수

<br>

# 2. Contextual Embedding

단어의 주변 "문맥"을 고려한 embedding을 수행한다!

- Input : Embedding vector ( $$d_{model} = 768$$ )

- Output :   문장의 "문맥"을 고려한 Embedding Vector ( 마찬가지로 768차원 )

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG" width="650" />.

<br>

위 그림에서 주황색으로 칠해진 12-layers 영역에서는 어떠한 작업이 이루어질까?

즉, 어떠한 식으로 주변 문맥을 고려한 embedding을 하는 것일까? 

아래의 그림을 통해 쉽게 이해할 수 있다 ( 이해가 안된다면 Transformer 포스트를 확인하기! )

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG" width="650" />..

<br>

# 3. BERT의 sub-word Tokenizer : WordPiece

BERT가 처리하는 text는

- word (X)
- sub-word (O)

 BERT가 사용하는 Tokenizer : WordPiece Tokenizer

( character부터 sub-word들을 병합해가는 방식으로 Vocabulary 생성 )

<br>

**WordPiece Tokenizer**

- 기본 Idea :
  - 자주 등장 O 단어 ) 단어 집합에 추가
  - 자주 등장 X 단어 ) 보다 작은 단위인 sub-word로 분리한 뒤 단어 집합에 추가

- *BERT의 단어 집합에 em, ##bed, ##ding, #s라는 서브 워드들이 존재한다면,* 

  *embeddings는 em, ##bed, ##ding, #s로 분리됩니다.* 

  *여기서 ##은 이 서브워드들은 단어의 중간부터 등장하는 서브워드라는 것을 알려주기 위해 단어 집합 생성 시 표시해둔 기호입니다.* 

  *이런 표시가 있어야만 em, ##bed, ##ding, #s를 다시 손쉽게 embeddings로 복원할 수 있을 것입니다.* ( 위키독스 )

<br>

# 4. Position Embedding

Transformer의 Positional Encoding :

- sin & cosine함수 사용
- 위치에 따라 다른 값을 가지는 행렬 생성
- 위치 정보를 알려줌!

<br>

Bert의 Position Embedding도, 위와 같이 sequential data를 한번에 처리하기 위해 추가적으로 위치 정보를 알려줘야 하지만, sin & cosine 함수가 아닌 조금 다른 방식을 사용한다.

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG" width="550" />.

위치 정보를 알려주기 위한 Embedding layer를 하나 더 사용!

BERT의 최대 문장 길이는 512이므로, 총 512개의 Position Embedding vector가 학습된다!

정리 :

- 512개의 word vector를 위한 embedding layer 
- 512개의 position vector를 위한 embedding layer

<br>

# 5. Pre-training

<img src="https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png" width="850" />.

**(오른쪽) ELMo**

- 양방향 LM 
- forward LSTM & backward LSTM을 각각 따로 훈련 

**(가운데) GPT-1**

- 단방향 LM
- Transfomer의 Decoder를 쌓아서 사용

**(왼쪽) BERT**

- 양방향 LM
- MLM (Masked Language Model)
- 2가지의 pre-train 방법
  - 1) Masked Language Model (MLM)
  - 2) Next Sentence Prediction (NSP) 

<br>

## 5-1. Masked Language Model (MLM)

Input Text의 15%의 단어를 random으로 Masking & 해당 Masked word를 예측하도록 학습!

엄밀히 말하면, 15%를 전부 Masking하는 것이 아니라, 아래의 pie chart와 같이 변형한다.

<br>

<img src="https://wikidocs.net/images/page/115055/%EC%A0%84%EC%B2%B4%EB%8B%A8%EC%96%B4.PNG" width="350" />.

<br>1) Example : 'My dog is cute. he likes playing'

2) 변형

- 'dog'  $$\rightarrow$$ [MASK] **( [MASK]으로 변경 후 예측 )**
- 'he' $$\rightarrow$$ 'king' **( 랜덤으로 변경 후 예측 )**
- 'play' **( 미변경 후 예측)**

3) 결과

- ( original ) ['my', 'dog', 'is' 'cute', 'he', 'likes', 'play', '##ing']
- ( masking )  ['my', **[MASK]**, 'is' 'cute', **'king'**, 'likes', **'play'**, '##ing']

<br>

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG" width="550" />.

<br>

## 5-2. Next Sentence Prediction (NSP)

[Q] 2개의 문장이 서로 이어지는지 여부를 예측하는 문제!

example

- Y = 1 ( 이어지는 문장 )
  Sentence A : The man went to the store.
  Sentence B : He bought a gallon of milk.
- Y = 0 ( 이어지지 않는 문장 )
  Sentence A : The man went to the store.
  Sentence B : dogs are so cute.

<br>

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG" width="550" />.

- [SEP] : 문장을 구분해주는 Token
- [CLS] 위치의 출력층에서,2개의 문장이 서로 이어지는지의 binary classification 문제를 푼다

<br>

## 5-3. Summary

5-1 & 5-2 : MLM과 NSP는 **따로 학습하는 것이 아니라, 두 loss를 합한 뒤 동시에** 학습!

<br>

BERT가 (5-1의) Language Model 외에도, 다음 문장을 예측하는 NSP 또한 학습하는 이유?

$$\rightarrow$$ BERT가 풀고자 하는 task로, **QA(Question Answering)나 NLI(Natural Language Inference)** 등 두 문장의 관계를 이해하는 것이 핵심인 task들이 있기 때문에!

<br>

## 6. Segment Embedding

BERT는 QA 등과 같은 2개의 문장 입력이 필요한 task 또한 푸는데에 사용된다. 

때문에, BERT는 문장 구분을 위해 Segment Embedding을 하기 위한 또 다른 Embedding layer를 사용한다.

- 첫번째 문장에는 Sentence 0 임베딩
- 두번째 문장에는 Sentence 1 임베딩

을 더해준다.

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG" width="550" />.

<br>

# 7. Fine Tuning

## 7-1. Single Text Classification



<img src="https://wikidocs.net/images/page/115055/apply1.PNG" width="350" />..

<br>

ex) sentiment classification, document classification....

문서의 시작에 [CLS] token을 입력

해당 위치의 출력층에서 Dense Layer를 추가하여 classification을 수행!

<br>

## 7-2. Tagging



<img src="https://wikidocs.net/images/page/115055/apply2.PNG" width="350" />.

<br>

## 7-3. Text Pair Classification or Regression



<img src="https://wikidocs.net/images/page/115055/apply3.PNG" width="550" />..

<br>

ex) 자연어 추론(Natural language inference)

- 2개의 문장이 주어졌을 때, 하나의 문장이 다른 문장과 논리적으로 어떤 관계에 있는지를 분류하는 것!

  $$\rightarrow$$ 모순 관계(contradiction), 함의 관계(entailment), 중립 관계(neutral)

  <br>

## 7-4. Question Answering

<img src="https://wikidocs.net/images/page/115055/apply4.PNG" width="550" />.

<br>

# 8. Attention Mask

- 불필요하게 padding된 부분에 불필요하게 Attention하지 않게, 실제 단어 & padding 부분을 구분해주는 입력

- 1 = 실제 단어 ( = Masking (X) )

  0 = padding ( = Masking (O) )

<img src="https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC11.PNG" width="550" />..

