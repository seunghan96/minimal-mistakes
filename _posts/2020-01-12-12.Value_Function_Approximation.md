---
title: 12.Value Function Approximation
categories: [RL]
tags: [Reinforcement Learning]
excerpt: Value Function Approximation
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 12. Value Function Approximation ]

## 1. Introduction
Planning(Dynamic) & Learning(MC, TD)을 통해 문제 해결 <br>
지금까지는 tabular method를 활용하여 value-function을 구해왔음. <br>
( tabular method : grid 형식처럼, 각각의 state가 하나의 grid가 되고, action은 상/하/좌/우 중 하나로 움직여서 인접한 grid로 state가 넘어가는 단순한 형태 )<br><br>
<img src="https://cs.stanford.edu/people/karpathy/img/mdpdp.jpeg" width="500" /> <br>
(  https://cs.stanford.edu/people/karpathy/img/mdpdp.jpeg )

하지만 state & action의 경우가 훨씬 많아지고 복잡해지면, 이를 적용하기 쉽지 않다. 
또한, 이를 **일반화(generalization)**할 수 있는 식을 찾기 위해 등장한 것이 **value-function approximation**이다.

<br>

## 2. Function Approximation
Function Approximation으로 value-function을 구한다!

**value function** : $$\hat{v}(s,w) \approx v_{\pi}(s)$$

**action-value function** : $$\hat{q}(s,a,w) \approx q_{\pi}(s,a)$$

위 식에서 parameter(혹은 weight) $$w$$를 update하는 것이 핵심이다.

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRM9VuP5wN_dbfc5Q3ntKBDfRwfZ7d4OtiX2fipykDzt6XNs3DM&usqp=CAU" width="750" /> 

https://encrypted-tbn0.gstatic.com/images



이를 통해 풀수 있는 문제는 위와 같이 크게 3가지가 있을 수 있다.

- 1 ) value function 찾기 ( input : s )
- 2 ) action-value function 찾기 ( input : s & a )
- 3 ) action-value function 찾기 ( input : s )



### (1) 장점
- 실제 존재하지 않는 data도 function을 통해 계산 가능
- 실제 data의 noise를 배제하고 학습 가능
- 고차원의 data도 효율적으로 저장 가능



### (2) finding parameters
우리가 근사할 모델로 다양한 모델을 사용할 수 있다 (선형회귀, Neural Net, Decision Tree... 등) 그 중 우리는 미분 가능한(differentiable) 선형회귀나, Neural Network를 사용할 것이다. 최근 들어 가장 좋은 성능을 내는 것이 **Neural Net**이 자주 사용되는 대표적인 방법이다. (간략한 소개 : layer&node로 구성되어 있는 model / back propagation을 사용하여 weight & bias를 update하여 최적의 값을 찾음. 자세한 것은 https://github.com/seunghan96/datascience/tree/master/Deep_Learning/Basic_of_NN 참고 ) 



### (3) Gradient Descent
Gradient Descent(경사하강법)을 이용하여 적절한 parameter값을 찾아나간다. (딥러닝의 기초적인 내용이기 때문에 구체적인 설명은 여기서 하지 않을 것이다) 이를 RL의 value-function에 적용하자면, update하는 대상은 true-value function이 되고, minimize해야 하는 objective function은 다음 식과 같이 true-value function과 approximated-value function의 MSE로 잡을 수 있다. 

$$J(w) = E_{\pi}[\{v_{\pi}(s) - v(s,w)\}^2]$$



## 3. Value-function Approximation

### a) Value-Function

Gradient Descent를 통해 value function ( = $$\hat{v}(S,w)$$ )을 update해보자.

$$\begin{align*}
\Delta w &= -\frac{1}{2}\alpha \bigtriangledown_w J(w)\\
&= \alpha \; E_{\pi}[(v_{\pi}(s) - \hat{v}(s,w)) \bigtriangledown_w \hat{v}(s,w)]
\end{align*}$$


( update할 target은 MC, TD 등에 따라 다르다 )



1 ) **Monte Carlo Learning**에 적용한다면 ...

$$\Delta w = \alpha (G_t -\hat{v}(S_t,w)) \bigtriangledown_w 
\hat{v}(S_t,w)$$

- return $$G_t$$는 true value인 $$v_{\pi} (S_t)$$에 대한 unbiased estimator이다
- 주로 local optimum에 수렴하게 된다 ( 이것은 model로 linear value function을 사용하든, non-linear value function을 사용하든 마찬가지이다 )



2) **TD(0)**에 적용한다면...

$$\Delta w = \alpha (R_{t+1}-\gamma \hat{v}(S_{t+1},w) - \hat{v}(S_t,w)) \bigtriangledown_w \hat{v}(S_t,w)$$

- TD-target인 $$R_{t+1}$$은 true value인 $$v_{\pi} (S_t)$$에 대한 biased estimator이다
- Linear TD(0)는 global optimum으로 수렴한다



3) **TD($$\lambda$$)** 에 적용한다면...

$$\Delta w = \alpha (G_t^{\lambda} -\hat{v}(S_t,w)) \bigtriangledown_w 
\hat{v}(S_t,w)$$

- $$\lambda - return$$ 인 $$G_{t}^{\lambda}$$는 true value인 $$v_{\pi} (S_t)$$에 대한 biased estimator이다

<br>

### b) Action-value function
Gradient Descent를 통해 action-value function ( = $$\hat{q}(S,w)$$ )을 update해보자.

$$\begin{align*}
\Delta w &= -\frac{1}{2}\alpha \bigtriangledown_w J(w)\\
&= \alpha \; E_{\pi}[(q_{\pi}(s,a) - \hat{q}(s,a,w)) \bigtriangledown_w \hat{q}(s,a,w)]
\end{align*}$$


( update할 target은 MC, TD 등에 따라 다르다 )

( a)와 비교했을 때, action이 추가된것 외에는 전부 동일하다 )



1 ) **Monte Carlo Learning**에 적용한다면 ...

$$\Delta w = \alpha (G_t -\hat{q}(S_t,A_t,w)) \bigtriangledown_w 
\hat{q}(S_t,A_t,w)$$



2) **TD(0)**에 적용한다면...

$$\Delta w = \alpha (R_{t+1}-\gamma \hat{q}(S_{t+1},A_{t+1},w) - \hat{q}(S_t,A_t,w)) \bigtriangledown_w \hat{q}(S_t,A_t,w)$$



3) **TD($$\lambda$$)** 에 적용한다면...

$$\Delta w = \alpha (G_t^{\lambda} -\hat{q}(S_t,A_t,w)) \bigtriangledown_w 
\hat{q}(S_t,A_t,w)$$



위에서 한 gradient descent는 모든 training data를 다 사용해서 한 방법이다. 이렇게 하지 않고, data를 몇 개의 batch로 나누어서 할 수도 있다 ( Batch RL ) 위와 과정은 동일하기 때문에 이에 대해서는 생략하겠다.