---
title: 10.(nlp) word2vec
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: word2vec
---

# Word2Vec

( 참고 : "딥러닝을 이용한 자연어 처리 입문" (https://wikidocs.net/book/2155) )

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<br>

# 1. Sparse Representation vs Distributed Representation

단어와 같은 범주형 변수를 표현하는 방법에는 크게 다음과 같이 2가지가 있다

- 1) Sparse Representation
- 2) Distributed Representation

1)은 우리가 흔히 잘 알 고 있는 "원-핫 인코딩" 방법으로 표현하는 것이다.

하지만, 이는 단어의 종류 수 만큼의 차원이 필요하다는 점과 (매우 크다...), 해당 vector에 0이 너무 많다 ( sparse vector )라는 단점이 있다. 이러한 표현은, 단어 간의 유사성을 반영하지 못한다는 단점 또한 있다.

따라서, 이를 보다 적은 차원으로 표현한 것이 Distributed Representation이다. 해당 표현의 핵심은, "축소된 차원의 공간 속에서 비슷한 위치에 있는 단어들은 실제로 그 의미적으로도 유사하다"는 것이다.

<br>

# 2. Word2Vec

word2vec은, 위에서 설명한 "sparse representation"을 "distributed representation"으로 만들어주는 "mapping"으로 생각하면 된다.

아래의 두 가지만 기억하면 된다.

- 1) 보다 **저차원**으로 표현하면서
- 2) 축소된 차원들이 해당 단어가 가진 **의미**를 잘 반영하도록

mapping하는 것이라고 생각하면 된다.

<br>

Word2vec에는 크게 2가지 방법이 있는데, 이것은

- (1) CBOW (Continuous Bag of Words)
- (2) Skip-gram  이다.

**CBOW**는 *"주변 단어(context vector)"들을 통해, 중심 단어를 맞히는 것*이 목적이라면,

**Skip-Gram**은 *"중심 단어"를 통해 주변 단어(context vector)"들을 맞히는 것*이 목적이다.

<br>

그림으로 표현하면, 아래와 같다.

<img src="https://www.researchgate.net/profile/Dima_Suleiman/publication/332543231/figure/fig1/AS:749763205009408@1555768886449/CBOW-and-Skip-gram-models-architecture-6.png" width="550" />.

일반적으로 Skip-gram이 CBOW보다 약간 더 좋은 성능을 가지는 것으로 알려져 있다. 

그렇다면, 각각의 방법이 구체적으로 어떠한 특징을 가지는지 알아보자.

<br>

# 3. CBOW (Continuous Bag-of-Words)

CBOW는 주변의 단어들을 통해 중간에 있는 단어를 예측하는 방법이다.

다음과 같은 예시를 통해서 한번 알아보자.

ex) "The fat cat sat on the mat"

여기서 "주변"의 단어를 어디까지 볼 건지를 결정하는 것이 "window"이다. 위의 예시에서 window가 2일 경우, 중심단어와 주변 단어 pair는 아래와 같다.

<img src="https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG" width="450" />.

<br>

이를 NN를 통해 모델링해보자.

Input으로 들어가는 것은 "주변 단어"이고, Output으로 나오게 되는 것은 "중심 단어"이다. 이를 도식화 하면 아래와 같다.

<img src="https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG" width="450" />.



위의 그림은 일반적인 딥러닝의 NN와는 다르게, 비선형 활성화함수가 따로 존재하지 않아서 엄밀히 따지면 딥러니이라 할 수는 없다. 따라서, 위의 hidden layer의 위치에 있는 layer를 projection layer(투사층)이라고 부른다. (혹은 lookup table)

Input으로는 $$V$$ 차원의 one-hot vector가 들어오게 된다. 이것이 projection layer로 mapping되면서 그 차원이 $$M$$차원으로 축소된다. 이것을 "embedding"이라고 한다. 이렇게 $$M$$차원으로 embedding된 projection layer는 다시 $$V$$차원으로 변형되어 우리의 target인 중심단어와 유사하고자 한다. 따라서, projection layer는 weight matrix에 곱해진 뒤 softmax를 통과하여 최종적으로 one-hot encoding된 output layer의 "1"값에 높은 확률을 부여하고자 학습된다.

따라서, 여기서 사용되는 손실함수는 아래와 같은 cross-entropy이다.

$$H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_{j} \log \left(\hat{y}_{j}\right)$$

<img src="https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG" width="450" />.



Back-prop을 이용하여, 위 손실함수를 최소화하는 방향으로 모델이 학습된다! CBOW는 이와 같이 매우 간단한 구조를 가지고 있다.

<br>

# 4. Skip-Gram 

Skip-Gram은, CBOW에서 input과 output이 서로 바뀐것에 불과하다.

위의 예문을 통해, 중심단어와 주변 단어를 표현하면 아래와 같다.

<img src="https://wikidocs.net/images/page/22660/skipgram_dataset.PNG" width="450" />.

<br>

그리고 이를 위와 같이 도식화하면 아래와 같다.

<img src="https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG" width="450" />.

