---
title: 10.(nlp) word2vec
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: word2vec
---

# Word2Vec 실습

이번 포스트에서는 word2vec에 관해 실습할 것이다. word2vec 이론에 관한 구체적인 내용은 ___ 포스트를 참고하면 된다.



## 1. Import libraries & data


```python
import re
from lxml import etree
from nltk.tokenize import word_tokenize,sent_tokenize
```


```python
targetXML = open('ted_en-20160408.xml','r',encoding='UTF8')

target_text = etree.parse(targetXML)
parse_text = '\n'.join(target_text.xpath('//content/text()'))
# <content> ~ </content> 사이 내용 가져오기
```

<br>

## 2. Data Preprocessing

(Audio), (Laughter) 등의 배경음 부분을 제거


```python
content_text = re.sub(r'\([^)]*\)','',parse_text)
```



문장을 tokenize 해준다


```python
sent_text = sent_tokenize(content_text)
```



7번 index를 가진 문장은 다음과 같다.


```python
sent_text[7]
```


    'Everybody used them.'



구두점을 제거하고, 대문자를 전부 소문자로 바꿔준다.


```python
normalized_text = []

for string in sent_text:
    tokens = re.sub(r'[^a-z0-9]+', ' ', string.lower())
    normalized_text.append(tokens)
```



하나의 문장을 단어 단위로 나눈 뒤 그것을 result라는 리스트에 담아둔다. 그 결과, 총 273424개의 문장이 단어 단위로 나뉜 상태로 리스트에 담겨져있음을 확인할 수 있다.


```python
result = [word_tokenize(sentence) for sentence in normalized_text]
```


```python
print(' Number of samples : ', len(result))
```

     Number of samples :  273424



첫 세 문장이 어떠한 형식으로 담겨있는지 확인해보자.

```python
for line in result[:3]:
    print(line)
```

    ['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']
    ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']
    ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']

이 단어들을 vector로 만들기 위한 준비는 이제 끝났다.

<br>

## 3. Word2Vec

우리는 gensim에서 제공하는 Word2Vec을 활용하여 위 단어들을 임베딩할 것이다.

Word2Vec이 input으로 받는 인자들은 다음과 같다.

- size = dimension of embedded vector
- window = size of context window
- min_count
- workers
- sg=0 : CBOW /  sg=1 : Skip-Gram



우리는 각 단어들을 100차원으로 임베딩할 것이다. context size는 5로 설정하고, CBOW 모델을 사용할 것이다. (구체적인것은 해당 포스트 맨 위에 적혀있는 링크 참조)


```python
from gensim.models import Word2Vec
model = Word2Vec(sentences=result,
                size=100, window=5, min_count=5, workers=4, sg=0)
```



'man'이라는 단어와 가장 유사한 단어가 무엇인지 확인해본 결과, 'woman'이 0.849의 유사도로 가장 높음을 알 수 있다.

```python
model_result = model.wv.most_similar('man')
model_result
```

    [('woman', 0.8488868474960327),
     ('guy', 0.7953285574913025),
     ('lady', 0.7730017304420471),
     ('boy', 0.744835376739502),
     ('gentleman', 0.7383678555488586),
     ('girl', 0.7321925163269043),
     ('soldier', 0.7277054786682129),
     ('poet', 0.690149188041687),
     ('kid', 0.6736900806427002),
     ('rabbi', 0.6579049825668335)]

<br>

## 4. Save & Load Word2Vec Model

위에서 학습한 weight들을 저장했다가 원하는 떄에 다시 재학습할 필요 없이 불러들일 수 있다.


```python
from gensim.models import KeyedVectors
model.wv.save_word2vec_format('w2v_english')
loaded_model = KeyedVectors.load_word2vec_format('w2v_english')
```


```python
model_result2 = loaded_model.wv.most_similar('man')
model_result2
```

    [('woman', 0.8488868474960327),
     ('guy', 0.7953285574913025),
     ('lady', 0.7730017304420471),
     ('boy', 0.744835376739502),
     ('gentleman', 0.7383678555488586),
     ('girl', 0.7321925163269043),
     ('soldier', 0.7277054786682129),
     ('poet', 0.690149188041687),
     ('kid', 0.6736900806427002),
     ('rabbi', 0.6579049825668335)]




```python

```
