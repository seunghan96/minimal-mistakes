---
title: 10.SARSA
categories: [RL]
tags: [Reinforcement Learning, SARSA]
excerpt: SARSA, N-step SARSA
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 10. SARSA ]

## 1. 복습
policy evaluation & improvement?
- **evaluation** : 각 state에서의 value function 찾기
- **improvement** : 각 state에서의 최적의 policy 찾기

<br>

how to find value function? (=evaluation)
- Planning 기법(환경에 대한 정보가 있을 때) 
  - **1) Dynamic Programming **
- Learning 기법(환경에 대한 정보가 있을 때) 
  - **2) Monte Carlo Method**
  - **3) Time Difference Learning (forward & backward view)**



지난 두 포스트에서는 다양한 TD Learning의 Prediction 과정에 대해 공부했었다 ( TD, N-step TD, TD($$\lambda$$) 등 ) 이번 시간에는, TD Learning의 Control 과정에 대해서 공부할 것이다. <br>



## 2. TD Control : SARSA
**SARSA** : Time Difference Learning에서, value function 대신 action-value function을 사용하고(model-free 해짐), epsilon-greedy improvement를 적용한 것이다(local-optimum 문제 해결).
<br>

식으로 표현하면 다음과 같다.

**1 ) Value Function을 사용한 TD**

$$V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma\;V(S_{t+1})-V(S_t))$$



**2) Action-Value Function을 사용한 TD**

$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (R_{t+1} + \gamma \;Q(S_{t+1},A_{t+1}) - Q(S_t,A_t))$$



그 이름이 SARSA인 이유는, 아래 식에서 볼 수 있듯 **S**(state),**A**(action), **R**(reward)가 나오고 그 이후에 다음 **S**(state'), **A**(Action')이 나오기 때문이다. 

<br>
<img src="https://t1.daumcdn.net/cfile/tistory/998852495A636E3F01" width="200" /> <br>

SARSA의 process는 다음과 같다. 
( action-value function이 사용되었고, 보다 나은 action을 하기 위한 ($$\epsilon$$-greedy improvement를 사용한) policy improvement과정이 추가됨을 알 수 있다. )

 <br>
<img src="https://t1.daumcdn.net/cfile/tistory/9997F2425A64220127" width="800" /> <br>
( 출처 : https://sumniya.tistory.com/ )

<br>

요약하자면, 매 time-step마다 다음과 같은 과정이 iterative하게 이루어진다

- 1 ) Policy Evaluation : **SARSA**
- 2 ) Policy Improvement : **$$\epsilon-greedy$$ policy improvement**



## 3. N-step SARSA
N-step SARSA는 **N-step TD에서 value function을 action-value function으로 대체한 것** 말고는 차이가 없다. 따라서, 앞에서 본 것과 마찬가지로 n=1일 경우 그냥 일반 SARSA이고, n이 무한대로가면 Monte Carlo Learning과 동일해진다.



**n=1**인 경우 : SARSA

$$q_t^{(1)} = R_{t+1} + \gamma \; Q(S_{t+1})$$



**n=$$\infty$$**인 경우 : MC

$$q_t^{(\infty)} = R_{t+1} + \gamma \; R_{t+2} + ... + \gamma^{T-1}R_T$$



**n=N**인 경우 : **N-step SARSA**

$$q_t^{(n)} = R_{t+1} + \gamma \; R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^n Q(S_{t+n})$$



따라서 N-step SARSA에서의 updating equation은 다음과 같다.

$$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (q_t^{(n)}- Q(S_t,A_t))$$



( Forward view와 Backward view도 앞서 배운 내용과 동일하다. )

### (1) Forward-view SARSA <br><br>
<img src="https://t1.daumcdn.net/cfile/tistory/9989724B5A641ADF1E" width="250" /> 

weight : $$(1-\lambda) \sum_{n=1}^{\infty}\lambda^{n-1}q_t^{(n)}$$

forward-view SARSA : $$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (q_t^{\lambda}- Q(S_t,A_t))$$



### (2) Backward-view SARSA <br><br>

$$E_t(s) = \left\{\begin{matrix}
\gamma\;\lambda\;E_{t-1}(s,a)\;\;\;\;\;\;\;\;\;\;\;\;\;\; otherwise\\ 
\gamma\;\lambda\;E_{t-1}(s,a)+1\;\;\;\;\; if,\;s= s_t,a=a_t
\end{matrix}\right.$$

<br>

$$Q(S_t,A_t) \leftarrow Q(S_t, A_t) + \alpha\;\delta_t\;E_t(s,a)$$

<br>when $$\delta_t = R_{t+1}+\gamma\;Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \;\;(= TD\; error)$$

