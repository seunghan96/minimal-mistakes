---
title: 7.Monte Carlo Learning
categories: [RL]
tags: [Reinforcement Learning, Monte Carlo Method]
excerpt: Monte Carlo Approximation, Monte Carlo Control
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 7. Monte Carlo Learning ]

앞으로의 두 포스트에서 살펴 볼 방법들은 **"Model-Free"**한 방법들이다.

( ***Model Free하다?*** )

- MDP transition에 대한 사전 정보가 없음!
- Reward에 대한 정보도 없음!



Model-Free한 대표적인 두 가지 방법에는 

- **1 ) Monte Carlo Learning**
- **2 ) Temporal-Difference Learning** 이 있다.



이 두 가지 Learning 방법들에 대해 각각의 

- **(1) Model Free Prediction** :  "Estimate" the value function of an unknown MDP
- **(2) Model Free Control** : "Optimize" the value function of an unknown MDP 

에 대해 알아 볼 것이다.

그 중, 이번 포스트에는 **Monte Carlo Learning**의 Prediction과 Control에 대해 알아볼 것이다.



## 1. Monte Carlo Approximation

많이 들 알고 있겠지만, Monte Carlo Approximation에 대해 간략히 설명하게 넘어가겠다. 한 문장으로 요약하자면, "MC는 **sampling**을 통해 해를 찾는다(근사한다)"는 것을 의미한다.



**Example**

  <img src="https://kr.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/65563/versions/2/screenshot.jpg" width="600" />  <br> <br>
( https://kr.mathworks.com/matlabcentral/ )

위 그림 처럼 파란색 점과, 빨간색 점으로 이루어진 좌표를 보자. 여기서 우리는 파란색 점이 있는 부분의 넓이를 어떻게 구할까? **y=x^2**의 식으로 보여서, 적분을 통해서 쉽게 계산할 수 있겠지만, **확률을 이용하여 계산(엄밀히는 approximate)하는 방법**이 있다. 임의의 점을 sampling하여, 해당 좌표에 아무렇게나 찍었을 때 그 점이 파란색 부분에 속할 확률을 구하면 (그 횟수는 충분해야할 것이다) $$25 \times 25 \times prob$$ 통해 해당 부분의 넓이를 구할 수 있을 것이다. 이 쉬워보이는 개념이 **Monte Carlo Approximation**이다. 

<br>
요약 : **sampling을 통해 추정하고자 하는 값에 근사한다**
<br><br>

## 2. MC Prediction ( ESTIMATING value function )
<img src="https://t1.daumcdn.net/cfile/tistory/99B101345A4A0DD617" width="640" /> <br>

<br>

위 식은, **각 state에서 받을 수 있는 Return값**들을 나타낸다.

또한, 앞서 배웠던 Dynamic Programming에서는 다음과 같은 식에 따라서 각 state의 value를 구했다.

$$v(s) = E[G_t \mid S_t =s] = E[R_{t+1}+\gamma v(S_{t+1})\mid S_t=s]$$

하지만 **환경/모델에 대한 정보가 없어서(model-free) 위와 같은 식을 구할 수 없을 때**, 우리는 Monte Carlo Approximation 방법을 사용하여, 아래와 같이 Return들의 **random sample값들을 평균**내어 value function을 근사한다.
<br>

우리는 data가 계속적으로 생겨나고 있는 경우, 이 data들의 incremental mean을 다음과 같이 구할 수 있다.

$$\begin{align*}
\mu_k &= \frac{1}{k}\sum_{j=1}^{k}x_j \\
&= \frac{1}{k}(x_k + \sum_{j=1}^{k-1}x_j) \\
&= \frac{1}{k}(x_k + (k-1)\mu_{k-1}) \\
&= \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})
\end{align*}$$  



위의 방식대로, MC방법을 통해 value function도 다음과 같이 update할 수 있다.

 $$V(S_t) \leftarrow V(S+t) + \frac{1}{N(S_t)}(G_t-V(S_t))$$

$$N(S_t) \leftarrow N(S_t) +1$$

<br>

( 위의 $$V(S_t)$$ 에서, $$\frac{1}{N(S_t)}$$부분 ( 계산된 error의 방향으로 얼마나 움직일지를 나타내는 정도 )를 고정된 상수 $$\alpha$$로 놓고 non-stationary한 문제로 풀 수 있다. )



## First Visit Method vs Every Visit Method

한번의 episode의 과정 속에서, agent는 하나의 state에 여러 번 방문을 할 수도 있다. 이럴 경우 해당 state에 대한 **update가 이루어지는 방식**에는 다음과 같이 두 가지가 있다. <br> <br>**(1) First Visit Method** <br>
한 번의 episode에서 해당 state를 **처음 방문**했을 때만 update가 이루어진다.<br><br>**(2) Every Visit Method** <br>
한 번의 episode에서 해당 state를 **방문하는 모든 때**에 update가 이루어진다.

 <br>

각각의 update가 이루어질 때 마다, Incremental Counter인 $$N(s)$$는 $$N(s) \leftarrow N(s)+1$$로, Incremental total return인 $$S(s)$$는 $$S(s) \leftarrow S(s)+G(t)$$로 바뀐다. 그리고 마지막으로 최종 value (=$$V(s)$$)는 $$S(s)/N(s)$$로 계산할 수 있는데, 누적된 Return값의 합을 (First Visit Method의 경우) episode 횟수로, (Every Visit Method의 경우) 총 방문 횟수로 나눠준다. 여기서 N(s)이 무한대로 가면, V(s)는 <a href="https://www.codecogs.com/eqnedit.php?latex=V_{\pi}(s)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?V_{\pi}(s)" title="V_{\pi}(s)" /></a>에 근사한다.
<br><br>

## 3. MC Control ( OPTIMIZING value function )

이전에 배운 Policy Iteration은 크게 (1) policy evaluation과 (2) policy improvement로 나눠졌다. 여기서 (1)policy evaluation (각 state의 value function을 구하는 과정)에, 그냥 위에서 배운 Monte Carlo Approximation을 적용하면 **Monte Carlo Policy Iteration**이 된다. <br> 

하지만 Monte Carlo 방법에도 여전히 다음과 같은 2가지 문제점이 존재한다. 

**(1) Value Function** <br>
기존에 policy iteration에서는 policy evaluation 과정에서 value function을 다음과 같이 MDP과정을 통해 구할 수 있었다. 

$$v(s) = R_s+\gamma \sum_{s'\in S}^{ }P_{ss'}v(s')$$

 <br>
하지만, **환경/모델에 대한 정보가 없는 상황**( 위 식에서의 $$P_{ss'}$$ 가 없음 ) 에서는 위 식을 풀 수가 없다. 

<br>

**(2) Local Optimum** <br>
MC에서는 모든 state에 대한 정보를 가지고 있는 것이 아니라, sampling을 통해 정보를 모으기 때문에 **local optimum에 빠지게 될 가능성이 있다.**



그래서 이 두 문제를 해결하기 위해서 나온 것이 **"Monte Carlo Control"**이다. Monte Carlo Control은 (1)번 문제를, value function 대신 **(1) action-value function ( = Q value function )**을 이용하여 해결하고, (2)번 문제를 해결하기 위해 **(2) exploration**을 이용하여 local optimum에 빠지게 될 위험을 줄인다. 자세한 내용에 대해서는 뒤에서 더 다룬다.

<br>

### a) Q-value Function 사용<br>

***"value-function의 문제 해결"***



MDP의 경우, 다음과 같은 식을 통해 최적의 행동을 찾았다.

**model of MDP**

$$\pi'(s) = \underset{a \in A}{argmax}R_s^a + P^a_{ss'}V(s')$$



하지만, MC의 경우 다음과 같은 식을 통해 최적의 행동을 찾는다.

**model-free**

$$\pi'(s) = \underset{a \in A}{argmax}Q(s,a)$$


MDP에서는 value function ( $$V(s)$$ )을 사용하여 policy improvement를 했었다. 하지만 이를 활용할 수 없는 model-free한 Monte Carlo method의 경우, value function 대신 action-value function( $$Q(s,a)$$ )를 사용하여 이 문제를 해결할 수 있다.

<br>

### 4-2) Exploration <br>

***"local-optimum의 문제 해결"***



- $$1-\epsilon$$ 의 확률로 greedy action
- $$\epsilon$$의 확률로 random action



$$\pi(a \mid s) = \left\{\begin{matrix}
\epsilon / m + 1 - \epsilon \;\;\;\; if \; a^{*} = \underset{a\in A}{argmax}\;Q(s,a)\\ 
\epsilon / m  \;\;\; \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; otherwise
\end{matrix}\right.$$



 ( $$m$$ : 할 수 있는 행동의 가지 수 )

MC는 전체가 아닌 일부 sample로만 진행을 하기 때문에 **local optimum에 빠질 가능성**이 존재한다. 이를 방지하기 위해, agent로 하여금 늘 최적의 선택( 즉, Q(s,a)를 최대화하는 Action )만을 하는 것(=exploitation)이 아니라, 가끔은 돌발적인 행동( 즉, 다른 길을 시도해서 다른 optimum이 있는지 더 탐색해라! )을 하게끔 하는데, 이를 **exploration(탐험)** 이라고 한다. Agent가 얼마나 exploitation과 exploration을 나눠서 할 지에 대한 값이 바로 $$\epsilon$$ 이다.



$$q_{\pi'}$$ 에 대해 최적인 $$\epsilon-greedy$$ policy $$\pi'$$는, 그 어떠한 $$\epsilon-greedy$$ policy $$\pi$$ 보다 크거나 같다. 이는 다음을 통해서 증명할 수 있다.



$$\begin{align*}
q_{\pi}(s, \pi'(s)) &= \sum_{a\in A} \pi'(a \mid s)q_{\pi}(s,a)\\
&= \epsilon / m \sum_{a\in A}q_{\pi}(s,a) + (1-\epsilon)\underset{a \in A}{max}\;q_{\pi}(s,a)\\
&\geq \epsilon / m \sum_{a\in A}q_{\pi}(s,a) + (1-\epsilon)\sum_{a \in A}\frac{\pi(a\mid s) - \epsilon /m}{1-\epsilon}q_{\pi}(s,a)\\
&= \sum_{a\in A}\pi(a \mid s)q_{\pi}(s,a)\\
&= v_{\pi}(s)
\end{align*}$$

<br>

## 5. Summary
지금 까지 배운 Monte Carlo를 요약하면, 다음과 같다. 

<br>
<img src="https://i.stack.imgur.com/Dvwsb.png" width="800" /> <br>
( https://i.stack.imgur.com/Dvwsb.png ) <br><br>
우선 value-function대신 **Q-value function (action-value function)**을 사용한다는 점과, **Monte Carlo Approximation** 방법을 이용하여 일부를 sampling하여 진행한다는 점, 그리고 마지막으로 agent가 action을 선택할 때 <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon" title="\epsilon" /></a>의 확률로 **exploration**을 하여 local optimum에 빠질 위험을 줄인다는 것이 이 방법의 핵심이라 할 수 있다.