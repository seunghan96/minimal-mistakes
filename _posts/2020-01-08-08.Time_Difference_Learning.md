---
title: 8.Time Difference Learning (1)
categories: [RL]
tags: [Reinforcement Learning, Time Difference Learning]
excerpt: Time Difference Learning, N-step TD
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 8. Time Difference Learning ]

Time Difference Learning도 Monte Carlo Learning과 함께 model-free하다 ( MDP transition과 reward에 대한 정보가 없다 ). TD Learning은 MC Learning과 비교했을 때 무엇이 다른지 한번 확인해보자!

## 1. Introduction

Monte Carlo Method에서는 update가 매 episode마다 일어났다. 하지만 **episode가 길면 길수록 update가 늦게 이루어지게** 된다. 이를 보대 **online**으로 하기 위해서 나온 것이 **"Time Difference Learning**이다. TD Learning에서는 value function에 대한 update가 매 episode가 아닌 **'매 time step'**마다 이루어진다.
<br>

Monte Carlo방법에서는 아래와 같은 updating equation으로 value function이 update되었다. 여기서에서 $$G_t$$는 episode의 종료시까지 얻은 모든 reward들을 현재가치로 discount한 것을 나타낸다. 

$$V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))$$

<br>
하지만, Time Difference Learning 방법에서는 Monte Carlo처럼 epsiode 종료시까지 모든 reward들을 대상으로 구하지 않고, 아래와 같이 **하나의 time step에 대한 것으로 reward를 설정**하였다. 따라서 TD (Time Difference)의 updating 식은 아래와 같다.

$$V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma\;V(S_{t+1})-V(S_t))$$
<br>

**용어 정리**

- TD Target : $$R_{t+1} + \gamma \; V(S_{t+1})$$ 

- TD error :  $$\delta_t = ( R_{t+1} + \gamma \;V(S_{t+1}) ) - V(S_t)$$
- 

TD의 전체 process를 요약하면 다음과 같다. 매 step마다 업데이트가 이루어짐을 확인할 수 있다.

<img src="https://t1.daumcdn.net/cfile/tistory/99F78B405A63763B03" width="800" /> <br>
<br>

## 2. MC vs TD
결론부터 요약하자면, Monte Carlo Method의 target은 unbiased되어있지만 high variance라는 단점이 있고, Time Difference Learning의 경우에는 **biased 되어 있지만 low variance**라는 장점이 있다. (Bias/Variance Trade-off) Value Function의 update 식에서, $$G_t$$ 대신 $$R_{t+1} + \gamma V(S_{t+1})$$이 사용된 것을 확인했다면, 아마 직관적으로 알 수 있었을 것이다.  (  고려하는 time step의 수가 적다 보니, 당연히 variance는 줄어들게 된다 )

하나의 episode가 끝날때까지 기다리 지않고 그 전에 update를 한다는 점에서, MC Learning에 비해서 biased된 estimate이라는 단점을 가지지만, 보다 빠르게 update할 수 있다는 장점이 있다.



- $$G_t = R_{t+1} + \gamma \; R_{t+2} + \gamma^2 R_{t+3} ... $$는 unbiased estimate of $$v_{\pi}(S_t)$$



- $$R_{t+1} + \gamma \; v_{\pi}(S_{t+1})$$ 는 unbiased estimate of $$v_{\pi}(S_t)$$ (TD의 True Target)
- $$R_{t+1} + \gamma \; V(S_{t+1})$$ 는 biased estimate of $$v_{\pi}(S_t)$$ (TD Target)

<br>

## 3. N-step TD

지금까지 위에서 이야기한 TD는, 엄연히 말하면 TD(0), 즉 "하나의 time step"만을 고려한 TD인 것이다. 이 time step를 무한대로 보낸다면, 이는 결국 MC와 동일해질 것이다.
앞서 보았듯 **MC와 TD에는 각각의 장.단점**이 있다. 그래서 이 **둘을 적절히 섞은 TD(n)**방법이 있다. 표현에서도 알 수 있 듯, time step을 n으로 잡는 것이다. 아래 식을 보면 쉽게 이해할 수 있다. 



**n=1**인 경우 : TD ( = TD(0) )

$$G_t^{(1)} = R_{t+1} + \gamma \; V(S_{t+1})$$



**n=$$\infty$$**인 경우 : MC

$$G_t^{(\infty)} = R_{t+1} + \gamma \; R_{t+2} + ... + \gamma^{T-1}R_T$$



**n=N**인 경우 : **N-step TD**

$$G_t^{(n)} = R_{t+1} + \gamma \; R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^n V(S_{t+n})$$



따라서 N-step TD에서의 updating equation은 다음과 같다.

$$V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)}- v(S_t))$$



다양한 n값과 alpha값(=learning rate)을 Random Walk를 통해 시험해 보았고 time step의 수(n) 마다 최적의 alpha값이 다름을 확인할 수 있었다.
<br>
<img src="https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile4.uf.tistory.com%2Fimage%2F99B32E335A1E3B7815C934" width="800" /> 
(  https://img1.daumcdn.net/)
<br>

절대적으로 좋은 time step을 찾기는 어렵다. 그래서 여러 time step를 고려하여 Time Difference Learing을 한 것이 바로 $$TD(\lambda)$$이다.