---
title: 2.Markov Decision Process (MDP)
categories: [RL]
tags: [Reinforcement Learning, Value Function, Bellman Equation, MDP]
excerpt: Value Function, Bellman Equation, Markov Decision Process
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 2. Markov Decision Process (MDP) ]

## 1. Value function
**어떤 행동이 좋은 행동인가?** 이를 판단하기 위한 지표!

- **보상(reward)** : Agent의 행동에 따라 받게 되는 것

- **할인율(discount factor, 감마)** : 미래의 받게되는 보상을 현재로 당겼을 때 할인하는 비율!

- **return(G, total discount reward)** : 미래에 받게되는 모든 reward까지 현재가치로 표현한 것. 이 G를 식으로 표현하면 다음과 같다.

    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty }\gamma^k R_{t+k+1}$$ 
  

- **가치(Value, Value Function)** : 현재 상태(s)에서, (앞으로) 기대되는 모든 return들의 합의 . 이 V를 식으로 표현하면 다음과 같이 됨을, 앞의 return값을 통해 알 수 있다.
   
$$V(s) = E[R(s0)+ \gamma R(s1) + \gamma^2 R(s2) + .... \mid s0=s]$$

<br>

## 2. Bellman Equation
Value function은 크게 두 부분으로 나뉠 수 있다.<br>

- 1) 지금(현재 t시점) 즉시 받는 보상값 (immediate reward, $$R_{t+1}$$)
- 2) 미래 (t+1 시점~)에 받게되는 보상값들 (discounted value of successor state, $$\gamma\; v(S_{t+1}) $$ )

직관적으로 생각하면 쉽게 이해되는 개념이다. 아무튼, Bellman Equation은 간단히 말하면 Value function을 저렇게 두 개의 reward로 나눠서 볼 수 있다는 것을 나타내는 등식으로, 뒤에서 MRP (Markov Reward Process)와 MDP(Markov Decision Process)에서 보다 자세히 이야기할테니 이 정도의 개념만 알아두고 가도 좋을 것 같다.

<br>

## 3. MDP (Markov Decision Process)
우리는 특정 상태(state) $$S_t$$가 다음과 같으면 **Markov Property**를 가진다고 한다.

$$P[S_{t+1} \mid S_t] = P[S_{t+1} \mid S_1,...,S_t]$$

이는, 다르게 말하면 미래의 상태($$S_{t+1}$$)는 지금 현재 상태($$S_t$$) 만으로도 충분하다는 것을 의미한다 ( sufficient statistic of the future )



### 1) Markov Reward Process

Markov Reward Process는 Markov Process의 각 state에 'reward' 개념이 추가된 것이다. <br>
이 process는 다음과 같은 4개의 표현 $$<S,P,R,\gamma>$$ 으로 나타낼 수 있다.

- 1 ) $$S$$ : State의 집합

- 2 ) $$P$$ : transition Probability
   ( 즉, 상태 s에서 다음 상태인 s'으로 이동할 확률의 집합으로 보면 된다 ) 
  이 집합의 각 요소는 $$P_{ss'}$$ = $$P(s'\mid s) = P(S_{t+1}=s'\mid S_{t}=s)$$ 가 된다

- 3 ) R : Reward의 집합 
  이 집합의 각 요소는 $$r(s) = E[R_{t+1}\mid S_t=s]$$가 된다

- 4 ) $$\gamma$$ : 할인율
  
  Markov Reward Process의 예시를 한번 보자.
  <img src="https://t1.daumcdn.net/cfile/tistory/99E9FD355AEB1E0927" width="550" /> <br>
  ( 출처 : https://t1.daumcdn.net/cfile/tistory/99E9FD355AEB1E0927 ) 

  <br>

  위 그림에서 Agent는 탐험을 통해 Room들을 지나서, Outside, Trap, 혹은 Treasure에 도착함으로서 모험을 종료한다. 이 그림을 보면, Markov Reward Process의 요소인 $$S$$(state의 집합, 즉 3개의 room, outside, trap, treasure)와, $$R$$ (reward. state별로 도착 시 얻게되는 reward가 차이가 있음을 알 수 있다), $$P$$(각 state에서 인접한 다음 state로 이동할 확률)을 알 수 있다.
  ( 할인율은 따로 표시되어 있지 않지만, 0.5라고 해보자 ) 
  
  <br>
만약 이 탐험가가 현재 Room1에 있고, Room1 -> Room2 -> Outside 순으로 탐험하였다고 하자. 그러면 이 사람이 받게 되는 discounted total reward는 $$-1 + (0.5)x(-2) + (0.5)x(0.5)x1 = -1.75$$이다. 이와 같은 방법으로 계속 value function( discounted total reward의 평균.기대값 )을 계산한다면, 그 경우의 수는 매우 많아서 (시간 복잡도 : O(n^3) ), 이를 해결하기 위한 다른 iterative solving method ( ex. Dynamic Programming, Temporal Difference Learning, Monte-Carlo Method )등이 있다.



### Bellman Equation for MRP

아까 Bellman Equation을 통해, Value Function을 두 가지의 reward로 나눠 생각해 볼 수 있다고 했었다 ( 1. 지금 당장 받는 reward + 2. 미래에 받게될 reward의 (discounted) 총합 )


왜 그렇게 되는지, 다음 식을 통해서 알 수 있다.

$$\begin{align*}
v(s) &= E[G_t \mid S_t =s]\\
&= E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \mid S_t = s]\\
&= E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...) \mid S_t = s]\\
&= E[R_{t+1} + \gamma G_{t+1} \mid S_t = s]\\
&= E[R_{t+1} + \gamma v(S_{t+1}) \mid S_t = s]\\
\end{align*}$$



위 식에서

- 1 ) 지금 당장받게 되는 Reward : $$R_{t+1}$$
- 2 ) 미래에 받게될 Reward : $$\gamma\; v(S_{t+1})$$

라는 것을 알 수 있다.



현재의 state를 $$s$$라고하고, 다음 state를 $$s'$$라고하면, 우리는 위 식을 다음과 같이 표현할 수 있다.

$$v(s) = R_s + \gamma \sum_{s' \in S}P_{ss'}v(s')$$



위 식은, matrix형태로 보다 간단히 나타낼 수 있다.

$$v = R + \gamma Pv$$

$$(I - \gamma P)v = R$$

$$v = (I-\gamma P)^{-1}R$$



### 2) Markov Decision Process
MDP는 MRP에 **action**이라는 요소가 추가된 것 외에 전부 동일하다.

(MRP) $$<S,P,R,\gamma>$$

(MDP)  $$<S,A,P,R,\gamma>$$<br>

- 1 ) $$S$$ : State의 집합 [MRP와 동일]
- 2 ) $$P$$ : transition Probability
   ( 즉, 상태 s에서 다음 상태인 s'으로 이동할 확률의 집합으로 보면 된다 ) 
  이 집합의 각 요소는 $$P^a_{ss'}$$ = $$P(s'\mid s,a) = P(S_{t+1}=s'\mid S_{t}=s, A_t =a)$$ 가 된다
- 3 ) R : Reward의 집합 
  이 집합의 각 요소는 $$r(s) = E[R_{t+1}\mid S_t=s,A_t=a]$$가 된다
- 4 ) $$\gamma$$ : 할인율



MDP에는 MRP와 다르게, 주어진 상태에서 취하는 행동의 distribution인 **정책(policy)**가 있다.
그 식은 다음과 같이 표현할 수 있다. 

$$\pi(a\mid s) = P[A_t=a\mid S_t=s]$$

<br>

따라서 우리는 이제 transition probability $$P$$와, Reward $$R$$의 notation을 다음과 같이 약간 바꿀 필요가 있다. ( action이라는 요소가 추가되었기 때문에! (policy) )

$$P^{\pi}_{s,s'} = \sum_{a\in A}\pi(a\mid s)P^a_{ss'}$$

$$R^{\pi}_{s} = \sum_{a\in A}\pi(a\mid s)R^a_{s}$$



### Summary

MRP는, 주어진 state 정보만을 고려한 Reward를 계산했기 때문에 **(1) value function** (혹은 state-value function, $$v(s)$$ )만이 있었다.

하지만, MDP는 state의 정보 뿐만 아니라, action도 고려(policy)했기 때문에,  state만을 고려한 **(1) value function** 뿐만 아니라, 행동까지 고려하여 Reward를 계산한 **(2) action-value function** (혹은 Q function, $$q(s,a)$$) 도 구할 수 있다.



**MDP 1) Value Function**

$$\begin{align*}
v_\pi(s) &= E_{\pi}[G_t \mid S_t=s] \\
&= E_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \mid S_t=s]\\
&= E_{\pi}[R_{t+1}+\gamma \; v_{\pi}(S_{t+1}) \mid S_t=s]
\end{align*}$$



(보다 간단한 notation으로)

$$v_{\pi}(s) = \sum_{a\in A} \pi(a\mid s)(R_s^a + \gamma \sum_{s' \in S} P^a_{ss'}v_{\pi}(s'))$$



**MDP 2) Action-Value Function**

$$\begin{align*}
q_\pi(s,a) &= E_{\pi}[G_t \mid S_t=s,A_t=a] \\
&= E_{\pi}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \mid S_t=s,A_t=a]\\
&= E_{\pi}[R_{t+1} + \gamma \; q_{\pi}(S_{t+1},A_{t+1}) \mid S_t=s, A_t=a]
\end{align*}$$



(보다 간단한 notation으로)

$$q_{\pi}(s,a) = R^a_{s} + \gamma \; \sum_{s' \in S} P^a_{ss'}v_{\pi}(s')$$ 



### Optimal Solution

따라서, 우리는 위의 각각의 value function과 action-value function을 최대화 시키는 $$\pi$$를 찾으면 된다! 

$$ v^{*}(s) = \underset{\pi}{max}\; v_{\pi}(s)$$

$$q^{*}(s,a) = \underset{\pi}{max}\; q_{\pi}(s,a)$$

위를 풀 수 있는 방법에는 여러 가지 방법이 있다 ( Value Iteration, Policy Iteration, Q-Learning, SARSA 등 ) 이에 대해서는 다음 포스트에서 확인해보겠다 :)