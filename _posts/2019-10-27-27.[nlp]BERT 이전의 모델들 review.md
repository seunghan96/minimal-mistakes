---
title: 27.(nlp) BERT 이전의 모델들 review
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: Neural Machine Translation
---

# BERT 이전의 모델들 review

1. Pre-trained Word Embedding
2. Pre-trained Language Model
3. Masked Language Model

<br>

# 1. Pre-trained Word Embedding

word embedding의 방법들 : Word2Vec, FastText, GloVe...

word embedding을 사용하는 방법

- 1) 가지고 있는 데이터로 처음부터 embedding을 학습하는 방법

- 2) 이미 학습된 embedding vector를 가져와 사용하는 방법

  ( if 현재 가지고 있는 데이터가 적을 경우! )

<br>

위 두가지 embedding의 문제점?

$\rightarrow$ **문맥을 고려하지 못한다**는 점!

<br>

Solution : **Pre-trained Language Model** ( 사전 훈련된 언어 모델 )... ex) ELMo

<br>

# 2. Pre-trained Language Model

방대한 text data로, 사전에 Language Model을 학습하고, 이 결과를 다른 task에 사용!

<br>

<img src="https://wikidocs.net/images/page/108730/image2.PNG" width="570" />.

ex) **ELMo** 

- biLM을 각각 따로 학습시킨 후에, 이렇게 pre-trained LM으로부터 embedding vector를 얻기.

  ( but 2018년 Transformer 등장 이후, LM 학습 시, RNN/LSTM등의 recurrent network대신 Transformer를 사용하기 시작 ) 

<br>

<img src="https://wikidocs.net/images/page/108730/image3.PNG" width="570" />.



( Trm = Transformer )

<br>

Trend of NLP 

- pre-trained LM을 만들고, 이를 사용하여 다른 task에 추가적으로 학습!

- 기존의 "순차적"인 LM에서, 양방향의 LM을 사용! ... **"Maksed Languange Model"**

<img src="https://wikidocs.net/images/page/108730/image4.PNG" width="570" />.

<br>

# 3. Masked Language Model

Input text 단어의 15%의 단어를 random하게 Masking 

$\rightarrow$  이처럼 Masking된 단어들을 예측하도록 학습!  **ex) BERT**