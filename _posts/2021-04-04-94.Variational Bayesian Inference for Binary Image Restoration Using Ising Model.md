---
title: 94.Variational Bayesian Inference for Binary Image Restoration using Ising Model
categories : [BNN]
tags: 
excerpt: Paper Review by Seunghan Lee
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Variational Bayesian Inference for Binary Image Restoration using Ising Model (2013)

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<br>

# Abstract

Topics in image analysis : ex )

- **1) "removal blur and noise"** $\rightarrow$ this paper's focus
- 2) image segmentation
- 3) motion detecting

<br>

This paper proposes...

- "removal noise in the binary image based on **VB method using Ising model**"

- restore image using **VECM** algorithm!

  ( + evaluation using PSNR & ELBO )

<br>

# 1. Introduction

**(1) Previous Works**

- 1) **Restore degraded image (with Gaussian noise)** - Geman and Geman (1984)

- 2) **Bayesian Image Restoration** - Besag et al (1991)

- Both 1) & 2)...

  - construct posterior distn
    ( observation : degraded image / latent variable : original image )

  - use Gibbs sampler to estimate posterior

- 3) **Iterative Conditional Modes (ICM)** - Besag (1986)

<br>

Evaluating **(intractable) posterior** : (1) MCMC, (2) Gibbs sampler, (3) MH-algorithm

$\rightarrow$ MCMC methods, not scalable! **use VI instead!**

<br>

**(2) Variational Inference **

- solve it as an "optimization" problem

- minimize KL-div ( = maximize ELBO )

<br>

**(3) Proposal**

- propose simple model to restore degraded **binary image**!

- **use VI to approximate posterior**

- to estimate model params...

  - adapt **empirical Bayes method in EM algorithm**!

    but, updating multiple params simulaneously is infeasible

  - $\rightarrow$ use **ECM** ( instead of original EM )

<br>

**(4) Contents**

- ch 2) Ising model & BLotzmann-Gibbs distn
- ch 3) VBI, MF approximation, CAVI
- ch 4) VEM ( VI version of EM algorithm )
- ch 5) ECM algorithm
- ch 6) evaluation using PSNR & ELBO

<br>

# 2. Reviews of Ising Model & MRF

### (1) Ising model 

- model for **binary variables** ( up(+1) & down(-1) )
  

- **(1) total energy :**

   $E=h \sum_{i} S_{i}-\beta \sum_{\{i, j\}} S_{i} S_{j}$
  

- **notation**

  - $\{i, j\}$ : represents that $S_{i}$ and $S_{j}$ are the neighborhood
  - $S_{i}$ : spin state of site $i$
  - $h$  :  interaction constant between "$S_{i}$" and an "external field"
  - $\beta$ : constant between $S_{i}$ and $S_{j}$ for all $i, j$
    

- **(2) pdf ( = Boltzmann-Gibbs distn ) :**

   $P(S)=\frac{1}{Z} \exp \left(-\frac{1}{k T} E\right)$

  - $Z$ : normalizing constant ( = partial function )
  - $k$ : Boltzmann constant
  - $T$ : temperature

<br>

### (2) MRF (Markov Random Field)

suppose original image($X$) is **MRF (Markov Random Field)**

Notation 

- $X_{i j}$ : intensity value of the original image at $(i, j)$ on $N_{1} \times N_{2}$ lattice $L$ 

- $N_{i j}$ : neighborhood of $X_{i j}$

  

3 conditions for $P(X)$ to become MRF :

- 1) Positivity : $ P\left(X_{i j}\right)>0 \forall X_{i j}$
- 2) Markovinity : $P\left(x_{i j} \mid \text { all points in lattice } L \text { except }(i, j)\right)=P\left(x_{i j} \mid N_{i j}\right), \forall(i, j) \in L$
- 3) Homogenity : $P\left(x_{i j} \mid N_{i j}\right)=P\left(x_{k l} \mid N_{k l}\right)$ if the configuration of $N_{i j}$ and $N_{k l}$ is the same.



Joint distn of MRF : $P(X)=\prod_{i=1}^{N_{1}} \prod_{j=1}^{N_{2}} P\left(X_{i j} \mid N_{i j}\right)=\prod_{i=1}^{N_{1}} \prod_{j=1}^{N_{2}} \frac{1}{Z_{i j}} \exp \left(-\frac{1}{k T} E_{i j}\right)$.

<br>

# 3. VBI

use Bayes rule to find posterior

## 3-1. KL-divergence

Goal : minimize KL-divergence ( = maximize ELBO )

- ELBO : $E L B O(q)=\mathbb{E}[\log p(y \mid x)]-K L(q(x) \| p(x))$

<br>

## 3-2. MF approximation

assumption : latent variables are independent 

( enable analytical computation, but **limit its flexibility!** )

<br>

## 3-3. CAVI

Simplify optimization problem using CAVI 

updating equation of $i^{th}$ latent variable $x_i$ :

- ELBO : $E L B O\left(q_{i}\right)=\mathbb{E}_{i}\left[\mathbb{E}_{-i}\left[\log p\left(x_{i}, x_{-i}, y\right)\right]\right]-\mathbb{E}_{i}\left[\log q_{i}\left(x_{i}\right)\right]+\text { const }$
- solution :  $q_{i}^{*}\left(x_{i}\right) \propto \exp \left(\mathbb{E}_{-i}[\log p(x, y)]\right)$

<br>

# 4. The Proposed Model & its VBI

## 4-1. Simple Ising model

notation : 

- original image = $X$ ( components : $x_i$)
- observation $Y$  ( components : $y_i$ )

**(1) Joint distn :**

- $p(x, y ; h, \beta, \eta) \propto \exp \left(\beta \sum_{i=1}^{N} x_{i} \sum_{j \in N_{i}} x_{j}+\eta \sum_{i=1}^{N} x_{i} y_{i}-h \sum_{i=1}^{N} x_{i}\right)$.

**(2) Prior :**

- $P(X)\propto \exp \left(-h \sum_{i=1}^{N} x_{i}+\beta \sum_{i=1}^{N} x_{i} \sum_{j \in N_{i}} x_{j}\right)$.

**(3) Likelihood : **

- $p(y \mid x ; \eta) \propto \exp \left(\eta \sum_{i=1}^{N} x_{i} y_{i}\right)$.

<br>

## 4-2. VBI for simple Ising models

( using solution of **CAVI** )

$\begin{aligned}
\log q_{i}^{*}\left(x_{i}\right) &=\mathbb{E}_{-i}[\log p(x, y ; \theta)] \\
& \propto \eta x_{i} y_{i}+\beta x_{i} \sum_{j \in N_{i}} \mu_{j}-h x_{i}
\end{aligned}$.

- $\theta=(h, \beta, \eta)$
- $\mu_{j}=\mathbb{E}_{q_{i}}^{q}\left[x_{i}\right] .$

<br>

( after exponential & normalization above equation .... )

$\begin{aligned}
q_{i}^{*}\left(x_{i}\right) &=\frac{\exp \left(x_{i}\left(m_{i}+L_{i}\right)\right.}{\exp \left(-\left(m_{i}+L_{i}\right)+\exp \left(m_{i}+L_{i}\right)\right.}
\end{aligned}$.

- $m_{i}=\beta \sum_{j \in N_{i}} \mu_{j}$
- $L_{i}=\eta y_{i}-h$

<br>

( expectation of $x_i$ )

$\begin{aligned}
\mu_{i}=\mathbb{E}_{q_{i}}\left[x_{i}\right] &=q_{i}^{*}\left(x_{i}=+1\right)-q_{i}^{*}\left(x_{i}=-1\right) \\
&=\tanh \left(\eta y_{i}+\beta \sum_{j \in N_{i}} \mu_{j}-h\right)
\end{aligned}$.

<br>

( **Updating Equation** )

$\mu_{i}^{(k+1)}=\tanh \left(\beta \sum_{j \in N_{i}} \mu_{j}^{(k)}+\eta y_{i}-h\right)$.

<br>

# 5. Choosing the parameters

Goal : find $\theta$ maximizing likelihood!

- $\log p(y ; \theta)=E L B O(q)+K L(q(x) \| p(x \mid y))$.
- variational density $q^{*}$ : above!
- maximize ELBO w.r.t params

<br>

## 5-1. EM & VEM algorithm

### (1) EM algorithm

**Purpose of EM algorithm :**

- find MLE  : $\underset{\theta}{\operatorname{argmax}} L(\theta ; y) \propto p(y ; \theta) .$

<br>

**Key point : Introduce latent variable $x$ !**

- $\begin{aligned}
  \log p(y ; \theta) 
  & \geq \sum_{x} q(x) \log \frac{p(x, y ; \theta)}{q(x)}=L(q, \theta)
  \end{aligned}$.

- instead of maximizing $\log p(y ; \theta)$ , maximize $L(q, \theta)$( = $Q$ function of EM )

  how? Iteratively! ( E & M step )

<br>

**Review of EM algorithm**

(1) E-step : compute $Q$ function

( $y$ : observed data, $x$ : latent variable )

​	$\begin{aligned}
Q\left(\theta ; \theta^{(t)}\right) &=\mathbb{E}_{p(x)}[\log p(x, y ; \theta)] \\
&=\sum_{x} p\left(x \mid y ; \theta^{(t)}\right) \log p(x, y ; \theta)
\end{aligned}$.

<br>

(2) M-step : maximize $Q$ function

- $\theta^{(t+1)}=\underset{\theta}{\operatorname{argmax}} Q\left(\theta ; \theta^{(t)}\right)$.

<br>

**General EM vs VI**

- General EM :  $p(x \mid y; \theta)$ takes simple form
- VI : approximate $p(x \mid y; \theta)$ using $q^{*}$ $\rightarrow$ use this in EM algorithm! **VEM**

<br>

### (2) VEM algorithm

(1) VE-step : compute $Q$ function

​	$\begin{aligned}
Q\left(\theta ; \phi^{(t)}\right) &=\mathbb{E}_{q^{*}}[\log p(x, y ; \theta)] \\
&=\sum_{x} q^{*}\left(x ; \phi^{(t)}\right) \log p(x, y ; \theta)
\end{aligned}$.

- where $\theta=(h, \beta, \eta)$ and $\phi=\left(m_{i}, L_{i}\right) .$ A
- $\phi^{(t)}$ is computed by $\left(h^{(t)}, \beta^{(t)}, \eta^{(t)}\right)$
  - $m_{i}^{(t)}=\beta^{(t)} \sum_{j \in N_{i}} \mu_{j}^{(t)}$.
  - $L_{i}^{(t)}=\eta^{(t)} y_{i}-h^{(t)}$.

<br>

(2) M-step : maximize $Q$ function

​	$\theta^{(t+1)}=\underset{\theta}{\operatorname{argmax}} Q\left(\theta ; \phi^{(t)}\right)$. ( same as general EM )

<br>

## 5-2. ECM algorihm

Problem of EM(VEM)?

- have 3 model params! complex to update three simultaneously!

- solution : use **ECM (Expectation/Conditional-Maximization) algorithm**

  ( update one parameter $\theta_{k}$, while holding the other parameters )

<br>

call **VEM + ECM = VECM**

<br>

VECM algorithm

(1) VE-step : compute $Q$ function

​	$Q(\theta ; \phi)=\sum_{i=1}^{N} Q_{i}(\theta ; \phi)$      ( where $Q_{i}(\theta ; \phi) =\mathbb{E}_{q_{i}^{*}}\left[\log p\left(x_{i}, y_{i} ; \theta\right)\right]=\sum_{x_{i}} q_{i}^{*}\left(x_{i} ; \phi\right) \log p\left(x_{i}, y_{i} ; \theta\right)$. )


Solution : $\begin{aligned} Q(\theta ; \phi) &=\sum_{i=1}^{N} \tanh \left(m_{i}+L_{i}\right)\left(\beta \sum_{j \in N_{i}} x_{j}+\eta y_{i}-h\right)-\sum_{i=1}^{N} \log \left(2 \cosh \left(\beta \sum_{j \in N_{j}} x_{j}+\eta y_{i}-h\right)\right)
\end{aligned}$

- $Q\left(h ; h^{(t)}\right) =-h \sum_{i=1}^{N} \tanh \left(m_{i}+L_{i}\right)-\sum_{i=1}^{N} \log \left(2 \cosh \left(\beta \sum_{j \in N_{j}} x_{j}+\eta y_{i}-h\right)\right)$.
- $Q\left(\beta ; \beta^{(t)}\right) =\beta \sum_{i=1}^{N}\left(\tanh \left(m_{i}+L_{i}\right) \sum_{j \in N_{i}} x_{j}\right)-\sum_{i=1}^{N} \log \left(2 \cosh \left(\beta \sum_{j \in N_{j}} x_{j}+\eta y_{i}-h\right)\right)$.
- $Q\left(\eta ; \eta^{(t)}\right)=\eta \sum^{N} \tanh \left(m_{i}+L_{i}\right) y_{i}-\sum^{N} \log \left(2 \cosh \left(\beta \sum x_{j}+\eta y_{i}-h\right)\right)$.



(2) CM-step : maximize $Q$ function

- $h^{(t+1)}=\underset{h}{\operatorname{argmax}} Q\left(h ; h^{(t)}\right)$.
- $\beta^{(t+1)}=\underset{\beta}{\operatorname{argmax}} Q\left(\beta ; \beta^{(t)}\right)$.
- $\eta^{(t+1)}=\underset{\eta}{\operatorname{argmax}} Q\left(\eta ; \eta^{(t)}\right)$.

<br>

# 6. Result

### Algorithm summary

![figure2](/assets/img/VI/neurips20-16.png)

<br>

use PSNR(Peak Signal-to-Noise Ratio) as a measurement!

- $P S N R=10 \log _{10}\left(\frac{M A X_{I}^{2}}{M S E}\right)$.



