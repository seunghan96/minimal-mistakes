---
title: 4.Dynamic Programming (2)
categories: [RL]
tags: [Reinforcement Learning, Dynamic Programming]
excerpt: Value Iteration
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# [ 4. DP (2) Value Iteration ]

## 1. Value Iteration
앞서서는 Policy Iteration에 대해 알아보았다 ( Policy Iteration은 크게 '각 state에서의 value function을 찾아내는' **(1) Policy Evaluation**과, '각 state에서 optimal policy를 찾아내는' **(2) Policy Improvement**로 이루어짐을 확인하였다 ) 

<br>

Dynamic Programming으로 MDP를 푸는 또 다른 방법인 Value Iteration은, 다음과 같은 프로세스를 가진다.

- **1) Initialize V(s)=0, for all s** <br>

- **2) Repeat until converge**  $$V(s) = R(s) + \underset{a \in A}{max}\; \gamma \sum_{a'}^{ }P_{sa}(s')V(s')$$
    **for all s. **

  <br> Policy Iteration과의 차이점은, **improvement 과정이 없다는 것**이다. Policy Iteration에서는 Evaluation을 통해 state의 value를 알아내고, improvement를 
   통해 최적의 policy를 찾아낸 반면, Value Iteration에서는 (value에 각 action을 취할 확률을 곱해서 summation을 하는 대신) **max값을 바로 현재 state의
   value로 취한다**. 훨씬 더 간단하다. 아래 그림을 통해 확인해보자.  <br>
  ( https://t1.daumcdn.net/cfile/tistory/99B8303A5A47731F10 ) <br>  
  <img src="https://t1.daumcdn.net/cfile/tistory/99B8303A5A47731F10" width="800" />  <br><br>
   위 그림을 통해서 알 수 있듯, 각각의 state는 해당 state에서 갈 수 있는 **4개의 state들 중 max value를 자신의 value로** 만든다. 이렇게 iteration을 여러 번
   반복할 경우, 아래 그림과 같이 value들이 update된다. <br> <br>
   <img src="https://t1.daumcdn.net/cfile/tistory/990D2B365A489D6C21" width="800" />  <br>



 ## 2. 정리
Policy Iteration과 Value Iteration과 같은 Dynamic Programming 방식은 엄청난 계산을 요한다. 위의 경우에는 4x4 grid이어서 간단해 보였을 수 있지만, **실제 세상에서는 이 방법을 통해서 문제를 푸는데에는 제약**이 있다. 이어지는 포스트에서는 이러한 문제점을 보완하는 **Monte Carlo Method**에 대해서 알아보자.

