---
title: 11.(nlp) GloVe
categories: [DL,NLP]
tags: [Deep Learning, NLP]
excerpt: GloVe
---

# GloVe(Global Vectors for Word Representation)

( 참고 : "딥러닝을 이용한 자연어 처리 입문" (https://wikidocs.net/book/2155) )

- 기본의 카운트 기반의 **LSA** & 예측 기반의 **Word2Vec**의 단점을 보완!
- word2vec와 비슷한 성능!

<br>

# 1. LSA & Word2Vec의 한계

- 1) **LSA** = DTM이나 TF-IDF 행렬과 같이 *"단어의 빈도수"를 기반으로* 한 방법론 

- 2) **Word2Vec** = 실제값 & 예측값의 오차를 줄여나가는 *"예측 기반"*의 방법론

- LSA는 단어의 의미를 유추하는 작업에서 성능이 떨어진다는 단점 (ex 왕:남자 = 여왕:?)이,

  Word2Vec는 임베딩 벡터가 window 크기 내의 단어만을 고려하기 때문에 corpus의 전체적인 통계정보를 반영하지 못한다는 단점이 있음

<br>

**이 둘의 단점을 보완하기 위해, 이 두가지 방법론을 모두 사용하는 GloVe!**

- LSA의 "단어 빈도수" : **Window based Co-occurence Matrix**
- Word2Vec의 "예측 기반" : **Loss Function**

<br>

# 2. Window based Co-occurence Matrix

 i 단어의 **윈도우 크기(Window Size) 내에서 k 단어가 등장한 횟수**를 i행 k열에 기재한 행렬

<br>

<img src="https://www.researchgate.net/profile/Yoshihiro_Oyama/publication/250123514/figure/fig3/AS:298331709427715@1448139230448/Creation-of-event-co-occurrence-matrices-from-User-1-commands-data-Each-element-of-event.png" width="550" /> <br><br>

# 3. Co-occurence Probability

동시 등장 확률 P(k | i) = 동시 등장 행렬로부터 특정 단어 i의 **전체 등장 횟수**를 카운트하고, 

**특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수**를 카운트하여 계산한 **조건부 확률**

<br>

<img src="https://miro.medium.com/max/4200/1*4fJDgA3IoWDKewEf5cB7TA.jpeg" width="900" /> <br>

# 4. Loss Function

(구체적인 내용 생략) "**임베딩 된 중심 단어**와 **주변 단어 벡터**의 내적이, 전체 코퍼스에서 동시 등장 확률이 되도록 만드는 것"

<br>

# 5. 실습 


```python
from glove import Corpus, Glove
```

- word2vec 실습에서 사용한 데이터로


```python
import re
from lxml import etree
from nltk.tokenize import word_tokenize,sent_tokenize

targetXML = open('ted_en-20160408.xml','r',encoding='UTF8')
target_text = etree.parse(targetXML)
parse_text = '\n'.join(target_text.xpath('//content/text()')) # <content> ~ </content> 사이 내용 가져오기

# (Audio), (Laughter) 등의 배경음 부분을 제거
content_text = re.sub(r'\([^)]*\)','',parse_text)

# 문장 토큰화
sent_text = sent_tokenize(content_text)

# 구두점 제거 & 소문자화
normalized_text = []

for string in sent_text:
    tokens = re.sub(r'[^a-z0-9]+', ' ', string.lower())
    normalized_text.append(tokens)
    
result = [word_tokenize(sentence) for sentence in normalized_text]
```


```python
corpus = Corpus()
corpus.fit(result,window=5)

glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)
```


```python
model_result1 = glove.most_similar('university')
```
