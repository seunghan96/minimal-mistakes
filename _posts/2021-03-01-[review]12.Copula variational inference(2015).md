---
title: 59.Copula Variational Inference (2015)
categories: [BNN]
tags: 
excerpt: Paper Review by Seunghan Lee
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Copula Variational Inference ( 2015 )

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Abstract

이 논문은 latent variable들 간의 **dependency를 부여**하는 general한 VI방법을 제안한다.

그러기 위해, **"copulas"**라는 것을 도입하는데, 이는 Mean-field 혹은 Structured approximation에서 사용되는 distribution들의 family들을 augment하는 역할을 한다.

이를 사용함으로써, posterior에 보다 나은 approximation이 가능하다. 또한, stochastic optimization을 통해 scalability도 얻는다.

이 Copula variational inference는 아래와 같은 장점들을 가지고 있다.

- 1) reduce bias
- 2) less sensitive to local optima
- 3) less sensitive to hyperparameters

<br>

# 1. Introduction

SVI (Stochastic Variational Inference)는, stochastic optimization을 사용하여 보다 complex하고, massive data set에 적용할 수 있는 VI 방법이다.

optimization을 tractable하게 유지하기 위해, 많은 알고리즘들은 mean-field family를 사용한다.하지만 이 방법은, efficiency를 얻는 대신 어느정도의 accuracy를 포기한다.. ( 왜냐하면 많은 latent variable들은 실제로 서로 dependent하지만, mean-field approximation은 이들 간의 독립을 가정하기 때문이다. )

<br>

이 논문은 COPULA  VI ( copula variational inference )를 제안한다. 이는 factorized distribution들 간의 learning dependency를 construct한다. 이를 사용할 경우의 장점은 위의 **Abstract**에서 설명했다.

이 논문이 주는 contribution은 아래와 같이 3가지가 있다.

- 1) A generalization of the original procedure in VI
- 2) Improving generic inference
- 3) Richer variational approximations

<br>

# 2. Background

## 2-1. Variational Inference

Variational Inference는 다음의 ELBO를 최대화하는 것이 핵심이다.

$$\mathcal{L}(\boldsymbol{\lambda})=\mathbb{E}_{q(\mathbf{z} ; \boldsymbol{\lambda})}[\log p(\mathbf{x}, \mathbf{z})]-\mathbb{E}_{q(\mathbf{z} ; \boldsymbol{\lambda})}[\log q(\mathbf{z} ; \boldsymbol{\lambda})]$$.

<br>

MFVI는 아래와 같은 factorized된 distribution을 가정한다.

$$q(\mathbf{z} ; \boldsymbol{\lambda})=\prod_{i=1}^{d} q_{i}\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right)$$.

( 이 가정은, 모든 latent variable이 서로 independent하다는 강한 가정이다. )

<br>

보다 sophisticated한 approach는 **Structured Variational Inference**로, latent variable들 간의 dependence를 반영한다.

이 논문에서는, copulas를 사용하여 dependency를부여한다. Structured VI는 특정 모델에 맞춤형인 모델이라면, 이 모델은 보다 general한 posterior dependency를 학습할 수 있다!

<br>

## 2-2. Copulas

**Mean Field distribution과 copula를 augment**하여,, 아래와 같은 variational family를 가정한다.

$$q(\mathbf{z})=\left[\prod_{i=1}^{d} q\left(\mathbf{z}_{i}\right)\right] c\left(Q\left(\mathbf{z}_{1}\right), \ldots, Q\left(\mathbf{z}_{d}\right)\right)$$.

- $$Q\left(\mathbf{z}_{i}\right)$$ : CDF of $$\mathbf{z}_i$$

- $$c$$ : joint distribution of $$[0, 1]$$ variables

  ( distribution $$c$$는 $$z$$의 copula라고 부른다 )

  ( = joint multivariate density of $$Q(\mathbf{z_1}),...,Q(\mathbf{z_d}))$$ )

직관적으로 위 식을 봤을 때, copula는 marginal information을 제거한 이후의 multivariate random variable의 information을 잡아낸다.

<br>

예시) bivariate Gaussian copula : $$c\left(\mathbf{u}_{1}, \mathbf{u}_{2} ; \rho\right)=\Phi_{\rho}\left(\Phi^{-1}\left(\mathbf{u}_{1}\right), \Phi^{-1}\left(\mathbf{u}_{2}\right)\right)$$.

- $$\mathbf{u_1}$$와 $$\mathbf{u_2}$$는 independent uniform distributid
- inverse CDF  $$\Phi^{-1}$$ of standard normal  transforms $$(\mathbf{u_1},\mathbf{u_2})$$ to  independent normal

$$\rightarrow$$ 이때의 Gaussian copula는 이 둘 간의 **Pearson Correlation**을 뜻한다!

<br>

### 2-2-1. Vine copulas

copula를 어떻게 잡을지는 쉽지 않다. 다만, 우리는 아래의 두 가지 특징을 가진 family of distirbution을 찾고 싶다.

- 1) easy to compute with
- 2) express a broad range of dependencies

많은 실험 결과로, **vine**이라고 불리는 copulas가 성공적임을 확인하였다.

( vine에 대한 자세한 설명은 논문을 참조하길 바란다. )

![figure2](/assets/img/VI/2015-1.png)

<br>

### 3. Copula Variational Inference

이 논문은 (1) accurate 하면서도 (2) scalable한 Variational Inference 방법인 **Copula Variational Inference (COPULA VI)**를 제안한다. Simplicity를 위해, mean-field factorization에 copula를 합친 모형을 아래와 같이 가정한다.

$$q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta})=\underbrace{\left[\prod_{i=1}^{d} q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}\right)\right]}_{\text {mean-field }} \underbrace{c\left(Q\left(\mathbf{z}_{1} ; \boldsymbol{\lambda}\right), \ldots, Q\left(\mathbf{z}_{d} ; \boldsymbol{\lambda}\right) ; \boldsymbol{\eta}\right)}_{\text {copula }},$$.

- $$\lambda$$ : mean-field parameter
- $$\eta$$ : copula parameters

<br>

위 식을 통해, 우리는 아래의 **augmented ELBO**를 maximize한다.

$$\mathcal{L}(\lambda, \eta)=\mathbb{E}_{q(z ; \lambda, \eta)}[\log p(\mathrm{x}, \mathrm{z})]-\mathbb{E}_{q(\mathrm{z} ; \lambda, \eta)}[\log q(\mathrm{z} ; \lambda, \eta)]$$.

<br>

COPULA VI는 아래의 step을 반복한다.

- 1) $$\eta$$를 fix한채, mean-field parameter $$\lambda$$를 푼다
- 2) $$\lambda$$를 fix한채, copula parameter $$\eta$$를 푼다

<br>

이와 같이 푸는 optimization방법은 minorize-maximization method에 속한다. 이에 속하는 대표적인 방법이 우리에게 친숙한 EM알고리즘이다. 또한, COPULA VI에서 inference를 하기 위해서는 단지 joint model $$p(x,z)$$만 specify하면 된다. 마지막으로, 위에서는 mean-field factorization을 했찌만, 더 나아가서 structured factorization으로까지 확장 가능하다.

<br>

## 3-1. Stochastic gradients of the ELBO

Stochastic optimization을 시행하기 위해, 우리는 (1) $$\lambda$$ 와 (2) $$\eta$$에 대한 stochastic gradient of ELBO가 필요하다.

( 참고로, COPULA VI objective는 low variance를 가진 efficient stochastic gradient를 가진다. )

<br>

### (1) mean field parameter $$\lambda$$에 대해 optimize

ELBO를 $$\lambda$$에 대해 미분한 값은 아래와 같다.

$$\nabla_{\lambda} \mathcal{L}=\mathbb{E}_{q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta})}\left[\nabla_{\boldsymbol{\lambda}} \log q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta}) \cdot(\log p(\mathbf{x}, \mathbf{z})-\log q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta}))\right]$$.

<br>

Latent variable $$\mathbf{z}$$가 discrete한 경우 :  sample from $$q(\cdot)$$

Latent variable $$\mathbf{z}$$가 differentiable한 경우 : "Reparameterization trick"

- random variable $$\mathbf{u}를 $$도입하여 $$s(\mathbf{u})$$로 표현

- $$\mathbf{z}=\mathbf{z}(\mathbf{u} ; \boldsymbol{\lambda})$$.

- rewrite ELBO : $$\nabla_{\lambda} \mathcal{L}=\mathbb{E}_{s(\mathbf{u})}\left[\left(\nabla_{\mathbf{z}} \log p(\mathbf{x}, \mathbf{z})-\nabla_{\mathbf{z}} \log q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta})\right) \nabla_{\boldsymbol{\lambda}} \mathbf{z}(\mathbf{u} ; \boldsymbol{\lambda})\right]$$.

  $$\rightarrow$$ variance reduction 효과가 있다.

  $$\begin{aligned}
  \nabla_{\mathbf{z}_{i}} \log q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta}) &=\nabla_{\mathbf{z}_{i}} \log q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right)+\nabla_{Q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right)} \log c\left(Q\left(\mathbf{z}_{1} ; \boldsymbol{\lambda}_{1}\right), \ldots, Q\left(\mathbf{z}_{d} ; \boldsymbol{\lambda}_{d}\right) ; \boldsymbol{\eta}\right) \nabla_{\mathbf{z}_{i}} Q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right) \\
  &=\nabla_{\mathbf{z}_{i}} \log q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right)+q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right) \sum_{j=1}^{d-1} \sum_{e(k, \ell) \in E_{j}: \atop i \in\{k, \ell\}} \nabla_{Q\left(\mathbf{z}_{i} ; \boldsymbol{\lambda}_{i}\right)} \log c_{k \ell \mid D(e)} .
  \end{aligned}$$.

<br>

### (2) copula parameter $$\eta$$에 대해 optimize

$$\nabla_{\boldsymbol{\eta}} \mathcal{L}=\mathbb{E}_{s(\mathbf{u})}\left[\left(\nabla_{\mathbf{z}} \log p(\mathbf{x}, \mathbf{z})-\nabla_{\mathbf{z}} \log q(\mathbf{z} ; \boldsymbol{\lambda}, \boldsymbol{\eta})\right) \nabla_{\boldsymbol{\eta} \mathbf{z}}(\mathbf{u} ; \boldsymbol{\lambda}, \boldsymbol{\eta})\right]$$.

( (1) 과 (2) 계산에 있어서 공통적으로 필요한 $$\nabla_z \text{log}p(\mathbf{x},\mathbf{z})$$는 automatic differentiation tool을 통해 계산할 수 있다. )

<br>

## 3-2. Computational complexity

vine factorization에서는 총 $$d(d-1)/2$$ 개의 pair copula가 있기 떄문에 ( $$d$$ : latent variable의 개수 ), 총 $$O(d^2)$$만큼의 복잡도를 가지고 있다.

( low rank (Gaussian) approximation : $$O(Kd)$$ )



## summary

지금까지 위에서 설명한 COPULA VI의 알고리즘은 아래와 같다.

![figure2](/assets/img/VI/2015-2.png)